{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This is the hands-on part of our introductory course of scientific visualization with ParaView, containing a number of exercises.</p> <p>There are also a few chapters with details on specific topics, such as scripting and improving the appearance of your renders.</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Paul Melis &amp; Casper van Leeuwen (SURF)</li> <li>Robert Belleman (University of Amsterdam)</li> </ul>"},{"location":"#conventions","title":"Conventions","text":"<p>Whenever there is a step to perform in an exercise this will be marked with a \u25b6 character. For example:</p> <pre><code>\u25b6 Please open file ... and do ...\n</code></pre>"},{"location":"appearance/","title":"Appearance","text":"<p>The default rendering method used in ParaView does not produce shadows, nor any other realistic lighting effects. In certain cases you might want to have an image that looks a bit nicer (for example, as cover image). So we'll look into a few ways of accomplishing more appealing images below.</p>"},{"location":"appearance/#shadows","title":"Shadows","text":"<p>By default, ParaView renders a 3D view using fairly simple (GPU-accelerated) rendering. For example, this saved screenshot of an isosurface from the CT-scan dataset shows no shadows, and only uses a single light source:</p> <p></p> <p>Such a rendering lacks clues that you as a viewer normally uses to understand shapes and spatial relations. Here is the same image with added shadows:</p> <p></p> <p>As you can hopefully see, such a rendering provides much more information. For example, the parts seen through the right eye are now clearly seen to be located quite far backwards, something that was hard to judge in the non-shadowed image, and you can judge how much overhang there is from the cheekbone.</p> <p>To enable such shadowed rendering we used the Raytracing modes in ParaView. These use more advanced rendering and lighting techniques to produce more realistic renderings. To enable this for the whole scene, select a filter and check <code>Enable Ray Tracing</code> under the Ray Traced Rendering property block (all the way at the bottom), and then check <code>Shadows</code>.</p> <p></p> <p>This will change the rendering mode and show shadows on objects, just like the image above.</p> <p>Slow interaction with ray tracing modes</p> <p>Note that rendering and interaction can become quite a bit slower when enabling ray tracing. This has an influence on the whole user interface, and might make it almost unusable. So be somewhat careful in tweaking the ray tracing parameters, such as <code>Samples Per Pixel</code>, to high values. </p> <p>In some cases it might be more effective to only use ray tracing mode when generating images from a Python script.</p> <p>In this particular case we used the <code>OSPRay raycaster</code>, which is a CPU-based rendering mode with a fairly simple lighting model. Rendering and interaction speed will depend on the number of CPU cores in your system.</p>"},{"location":"appearance/#path-tracing","title":"Path tracing","text":"<p>A more advanced rendering method is Path tracing, which simulates more realistic lighting effects, such as light bouncing between surfaces reaching otherwise dark areas. Shadowing is by definition a part of the lighting when using path tracing, so the <code>Shadows</code> checkbox is greyed out.</p> <p>There's two path tracer modes in Paraview, the OSPray path tracer and the OptiX path tracer. The former (OSPray), is always available and is CPU-based, while the latter (OptiX) is GPU-based and only available if your system has an NVIDIA GPU. So the latter is to be preferred if you can enable it, as it will lead to less interaction slowdown. The resulting images will differ somewhat in lighting between these rendering modes, by the way.</p> <p>With pathtracer rendering the main setting to tweak is <code>Samples Per Pixel</code>. The higher you set this the less noisy and better the rendered image becomes (but remember the warning from above about slow interaction). For example, here's the isosurface as shown above, but now rendering with the OptiX pathtracer and 4 samples per pixel (left), or even less noisy, when using 16 samples per pixel (right):</p> <p> </p> <p>Compare the image using the shadowed raycasting we initially showed above, versus the path tracing image:</p> <p> </p> <p>Much more light can be seen reaching the depth of the eye socket, and the shadows is general are less harsh. The reason for the difference in surface color is not just due to different lighting effects, but also because of a different surface material used by default between these modes. </p> <p>Settings controlling material and appearance can be changed after enabling the advanced properties using . ParaView comes with a list of pre-defined materials, for example, here's what we get when selecting <code>Metal_Copper_Flat</code> under Material (in the Ray Tracing section):</p> <p></p>"},{"location":"appearance/#volume-rendering","title":"Volume rendering","text":"<p>We briefly showed volume rendering in Exercise 1, but actually did not go into the most important part, setting the transfer function. The transfer function is used to control the appearance, in terms of color and opacity, of a volume. Simply speaking it maps a voxel data value to a (color, opacity) pair. By doing this for all voxels and compositing the visual result back-to-front an image is generated. It is a really powerful technique for visualizing volumetric data, but it can be a bit challenging to get good results.</p> <p>The default transfer function in ParaView is linear, mapping the data range minimum value to fully transparent blue, and the data range maximum value to fully opaque red. See the graph representation and color bar in the Color Map Editor (<code>View &gt; Color Map Editor</code>):</p> <p></p> <p>This default gives a fairly good overview of what's in the full dataset value range. But, for example, the values corresponding to the skull, which we know from the contour used earlier is around 2000, are mostly hidden by many layers of voxels containing the skin. We can just barely see a bit of the skull near the right eye show through.</p> <p>We can try to show the bone more clearly by making the values 2000 opaque, and lowering the opacity of the existing points. By clicking in the graph we add a point, while dragging points allows control over both associated value (X) as well as opacity (Y):</p> <p></p> <p>Unforunately, this hasn't helped much, although the rendering is slightly different. We're still seeing too much of the low values (blue). Let's add another point to bring down the low value range to complete transparency:</p> <p></p> <p>Now we're getting somewhere, it nicely shows the skull (and other higher-valued parts, like the vertebrae and teeth). The skull is shown almost fully white, though, with no good way to grasp its shape (similar to lack of shadows showed earlier). This is due to the simple rendering used by default for volume rendering: it only composites mapped voxel colors onto each other, there's no advanced lighting being applied. Luckily, there's a setting to improve this. We can enable <code>Shade</code> in the Volume Rendering section:</p> <p></p> <p>We can further tweak the colors of the different values in the transfer function, by clicking on a point in the color bar, pressing <code>Enter</code> and picking a more bone-like color:</p> <p></p> <p>By tweaking the transfer function a bit more we can get a nice view of the bone parts, in the context of the skin, and even highlighting the teeth a bit in white:</p> <p></p> <p>As you can see there's a quite lot that can be achieved by editing the transfer function. In this case we used a one-dimensional function, mapping data value to color + opacity. Recent versions of ParaView have added the ability to use a two-dimensional transfer function, which has data value and associated gradient value as input. This allows finer control over appearance of specific parts of a volume.</p>"},{"location":"bonus/","title":"Bonus Exercise - A stationary fluid mixer","text":"<p>You should now have gained enough ParaView skills in order to re-create the visualization on the Home page of these course notes:</p> <p></p> <p>The visualization shows a so-called static (or stationary) mixer. In such a mixer two fluids enter separated on one side and exit mixed on the other side. The basis for the visualization is the data file SMRX.vtk.</p> <p>Good luck!</p> <p>Tips</p> <ul> <li>After loading the data use the Information tab to see what is actually in the dataset.</li> <li>Before applying any filters use the available representations in the toolbar to get some initial insight into the data.</li> <li>Try to figure out which filters you need to produce the visual elements shown in the picture on the title page.</li> </ul>"},{"location":"exercise1/","title":"Exercise 1 - CT scan of a boy","text":""},{"location":"exercise1/#first-steps","title":"First steps","text":"<p>In the first exercise we are going to take a look at a CT scan of a boy's head. A CT scanner produces a set of two dimensional X-ray images (usually called slices) that together form a 3D volume when stacked on top of each other. The 3D volume created in this way contains a density value for each voxel of the area scanned. As each part of the body (bone, tissue, blood, etc) has a different density w.r.t. X-rays these parts can be isolated and visualized using the density values.</p> <p>The boy in this CT scan was sent to the hospital for a reason, and as part of the exercise you are encouraged to use ParaView to investigate and visualize the data set, to see if you can find out why he was in hospital.</p> <p>\u25b6 To begin, start the ParaView application. How to start ParaView depends on where you installed the binaries after downloading them.</p> <p>\u25b6 Once ParaView has opened, you have to load the first data file. To do this, you can use the open file icon in the toolbar, or File \u2192 Open in the menu. Navigate to directory where you put the data, select the <code>headsq_masked.vtk</code> file and open it.</p> <p>The pipeline shown in the Pipeline browser in the upper-left corner of the ParaView window should now show the loaded data file:</p> <p></p> <p>\u25b6 Next we want to apply the changes to this pipeline (i.e. the actual loading of the data), by pressing Apply in the Properties tab, directly below the pipeline browser.</p> <p>Lazy pipeline update</p> <p>Remember that every time that you make changes to the pipeline or a filter's parameters, you have to \"Apply\" to make the pipeline execute.</p> <p>The reason for this is that a pipeline update can take quite some time, depending on the data size and operation. The manual apply allows you to make several changes to the filter first before triggering a pipeline update.</p> <p>\u25b6 Now we want to have a quick look at the data. By default, newly loaded items are made visible in the current view area on the right, and in the pipeline browser on the left you should see a small eye icon \ud83d\udc41 next to the headsq_masked.vtk object. By toggling the eye you can toggle visibility, a useful feature to control what elements you want to see.</p> <p>\u25b6 Another handy thing to check is the metadata in the Information tab, directly below the pipeline browser. It shows you the type of the dataset, the number of points and cells, data ranges, spatial extent, data memory size, etc.</p> <p>\u25b6 Try to get a bit of a feel for the mouse interaction in the 3D view (LMB = rotate, MMB = drag, RMB or wheel = zoom).</p> <p>Right now, your data is visible only as an outline: a box made from a few white lines representing the spatial extent of your data. This does not give us much insight in the data, so we want to switch to something more suitable for the moment. </p> <p>\u25b6 Use the visualization drop down menu in the toolbar to make a quick change to the visualization type. You can play around with the different representations available, with Slice and Volume being good ones for this type of data. When using Slice representation the slice direction and position can be controlled using the Slicing settings in the Properties tab (the latter is directly below the pipeline browser on the left).</p> <p></p> <p>Volume representation 'Are you sure?'</p> <p></p> <p>When enabling Volume representation for the first time in a session, a dialog box may pop up asking if you are sure, which you can answer with Yes. For large datasets volume rendering can be problematic if your system doesn't have a lot of memory and/or has a slow GPU. For the datasets used here volume rendering on most systems should not be a problem.</p> <p>Especially in Volume representation you can clearly see that indeed we are looking at the scan of a (boy's) head. </p> <p>\u25b6 Switch to Slice representation and enable display of the color legend with either the Show button under Coloring in the properties, or using the Toggle Color Legend Button:</p> <p></p> <p>You should now see a color scale in the view area that shows how scalar values in the data are mapped to colors. </p> <p>\u25b6Vary the slice position and orientation within the dataset using the Slicing properties (Slice Direction and Slice).</p>"},{"location":"exercise1/#isosurfacing","title":"Isosurfacing","text":"<p>Our next job is to look into what this dataset contains. To be more precise, we want to explore certain features of the dataset in order to look for things out of the ordinary. For example, we are interested in physical structures such as bone and skin.</p> <p>\u25b6 To do this, we first need to add a Contour filter to the pipeline: select the data source (headsq_masked.vtk) in the pipeline browser and add a Contour filter. You can do this by either using the icon symbol in the toolbar (half a sphere) or use Filters \u2192 Common \u2192 Contour in the menu bar:</p> <p></p> <p>Quick search to add a filter</p> <p>When you know the name of the filter you want to add, a quicker way than navigating the Filter menu is to use Ctrl+Spacebar. This will show a search dialog where you can type (part of) the filter name and matching filters will get listed. Select the filter you want and press Enter to add it.</p> <p>Of course, make sure to have selected the filter whose input you want to use before adding the downstream filter.</p> <p>\u25b6 Next, we want to see what this filter does: in the Properties tab, set the value of the contour under Isosurfaces to 500 and click on Apply and you should see a surface more-or-less representing the boy's skin.</p> <p></p> <p>\u25b6 To get an even better view we can hide the slice view by clicking on the relevant eye icon in the pipeline browser. You should now see the contour surface by itself. </p> <p>Another thing you might want to do is play with the color settings. For example setting the color of the contour surface to something more skin-like than gray might be an improvement. </p> <p>\u25b6 To alter the color click select Solid Color under Coloring (directly below the isosurface value settings), click the Edit button and select the color: </p> <p></p> <p>Next we want to add some more contour surfaces representing certain features in this dataset, such as bone. To do this, we want to add a few more iso-surfaces to our contour plot.</p> <p>\u25b6 Use the add value button (the +) of the Contour filter's Isosurfaces section, fill in some extra values and hit Apply when done. </p> <p></p> <p>Hint</p> <p>Try, for example, isovalues 1650 and 2750.</p> <p>Note that although we now have multiple surfaces, one for each scalar value, they all have the same color, making them hard to distinguish. We can assign a separate color for each scalar value to overcome this. </p> <p>\u25b6 Under Coloring change the coloring mode to Scalars. Check the color scalar against the color of the surfaces shown.</p>"},{"location":"exercise1/#transparency","title":"Transparency","text":"<p>\u25b6 The surfaces are now a bit easier to comprehend, although you might have to search a bit for the surface for the highest scalar value (check the color scale what color it should be). </p> <p>We can improve things even further by using a bit of transparency.</p> <p>\u25b6 Set Opacity to a lower value, for example, 0.3:</p> <p></p> <p>\u25b6 Finally, play a bit around with the viewpoint and inspect the visualization closely, and you should be able see why this boy was in hospital...</p>"},{"location":"exercise2/","title":"Exercise 2 - A Tornado simulation","text":""},{"location":"exercise2/#importing-and-preparing-the-data","title":"Importing and preparing the data","text":"<p>In this exercise we will take a look at the results of simulating the wind in a tornado. The relevant dataset contains velocity vectors on a regular 3D grid (a so-called flow field). The data is contained in a simple file format: the well-known comma separated values format (CSV). </p> <p>The advantage of CSV is that it is a simple and human-readable format, and can be written and read by many applications. However, it is not always an efficient way of storing (large) datasets. </p> <p>The data contained in the CSV file is not automatically identified and understood correctly by ParaView, it simply does not have enough information for that. Therefore when we open a CSV file, we first have to apply some filters to map the data to the correct data arrays that we can operate on. After this is done, we can use a stream-tracer filter on it to get a good impression of the wind flow in this simulation.</p> <p>\u25b6 The first thing to do is to create a new session, and start with a clean view. For this, use the disconnect button in the toolbar and pressing Yes (or the File \u2192 Disconnect menu option). This will completely clean your current pipeline, so you can start the next exercise:</p> <p></p> <p>\u25b6 Open the wervel.csv file and click Apply. ParaView will ask which reader to use, pick CSV Reader and click Ok. Next, use Apply to actually load the data. A new view pane with a table representation of the data will get added next to existing the 3D view. The table is similar to an Excel sheet:</p> <p></p> <p>If you would open the CSV file in a text editor you would see the file contains 7 values per line, with each line representing one grid point. </p> <p>These values are:</p> <ul> <li>A unique identifier per point, i</li> <li>x, y, and z values defining a location on a 3D cartesian grid.</li> <li>vx, vy and vz values, representing a 3D velocity vector at the (x,y,z) location</li> </ul> <p>\u25b6 Verify that the ParaView spreadsheet view indeed shows the same set of data.</p> <p>ParaView does not know by itself how these individual variables relate to the points and cells in its data model, so we have to provide that mapping manually.</p> <p>\u25b6 The first step is to add a TableToPoints filter to our data source. This filter creates a 3D point for each row in the table, based on a set of table values you choose. You can find this filter under Filters \u2192 Alphabetical \u2192 TableToPoints. Set the X, Y and Z columns correctly to the table columns representation point position, check Keep All Data Arrays and hit Apply.</p> <p></p> <p>\u25b6 Compare the Information tabs of the wervel.csv source and the TableToPoints filter to see how the output data has changed (type, number of points/cells, etc), and verify that the data arrays produced by the TableToPoints filter match those of the wervel.csv source.</p> <p>\u25b6 If needed, enable visibility of the TableToPoints output by clicking the eye icon next to it in the pipeline browser. If you have both a RenderView (the 3D panel) and a Spreadsheet view then make sure you have the RenderView selected (by clicking in it) before enabling visibility of the TableToPoints output. You should now see a regular 3D grid of (white) points, indicating that the point position values from the CSV data have correctly been set based on the table input.</p>"},{"location":"exercise2/#creating-velocity-vectors","title":"Creating velocity vectors","text":"<p>Next, we need to combine the three separate scalar values vx, vy and vz into one 3D vector value. To do this, we use the Calculator filter that is built into ParaView. </p> <p>Note</p> <p>There is also a Python Calculator filter, which is more powerful, but we don't use it here as it is somewhat buggy for what we want to do here.</p> <p>\u25b6 With the TableToPoints filter selected add a Calculator filter (Filter \u2192 Common \u2192 Calculator). In the Result Array Name field enter \"Velocity\", in the text box below that enter the exact expression shown and hit Apply. This will create a Velocity data array that holds the vector value (vx,vy,vz) for each point. Again, check the filter output data on the Information tab to verify this.</p> <p></p> <p>Warning</p> <p>A common mistake is to enter the expression in the Search box above the calculator properties:</p> <p></p> <p>This will cause properties to get filtered on the entered text (which there aren't), hiding them all.</p> <p>You will probably have understood the name iHat to represent the vector \u00ee, i.e. (1, 0, 0). Using the Calculator filter fairly complex expressions can be used to augment existing datasets with new and useful values, both scalars and vectors.</p> <p>\u25b6 Select the Calculator filter in the pipeline and add another Calculator filter, for creating an array VelocityMag and expression <code>mag(Velocity)</code>.</p> <p></p> <p>\u25b6 The pipeline we built up to this point, shown above, creates a Polygonal Mesh dataset which contains only points: the input point positions with their respective 3D velocity vectors. You can see this on the Information tab of the Calculator filter in the pipeline browser. Note that there's only 1 cell, and it contains all the 25,000 points. We also added two quantities derived from the input data, a velocity vector and its magnitude.</p>"},{"location":"exercise2/#visualizing-the-flow-field","title":"Visualizing the flow field","text":"<p>Now that our data is converted from the CSV input to the ParaView data model we can start looking at our flow field. To do this, we want to use the stream-tracer filter. With this filter we simulate injecting particles in our flow field at specific locations. By letting these particles follow the flow, based on the velocity vectors in the grid, and tracing them over time we get an impression of the fluid flow through the model. </p> <p>The stream-tracer filter is based on tracing a virtual particle through the cells of a dataset, i.e. regions of model space defined using 3D points. The flow direction within a cell, as indicated by the velocity vector value, is then integrated by the filter to determine a particle's position over time, thereby creating a trace of the particle through the dataset. </p> <p>But as noted above, there's currently only a single cell in our dataset holding all the points, which provides no meaningful way to trace particles. So we need to divide up the dataset domain into small cells. Furthermore, the flow vector values are currently associated only with the points of the dataset, while we need those values for each cell for the stream-tracer to work.</p> <p>We can fix these two issues by applying a 3D Delaunay triangulation. This creates cells from the dataset based on the existing points. The cells created are small and detailed enough so that our Stream Tracer filter can reasonably work. The filter also adds a new flow vector value for each created cell, based on interpolating the existing per-point values of its corners.</p> <p>\u25b6 Select the Calculator2 filter and add a Delaunay 3D filter using Filters \u2192 Alphabetical \u2192 Delaunay3D and click Apply. It might take a few seconds to a minute for the computation to complete.</p> <p>The representation of the data in the 3D view will now have changed to a block, instead of points, indicating that there are now cells which take up regions of 3D space.</p> <p>\u25b6 Check the Information tab for the Delaunay 3D filter and switch to Surface with Edges representation to see how the data has changed, in terms of type, cells and points. You could also clip away part of the dataset with the Clip filter to look at the cell structure inside of the dataset.</p> <p>The pipeline we constructed so far should look like this:</p> <p></p> <p>Now lets do some initial particle tracing through the flow field using the stream tracer filter.</p> <p>\u25b6 In the pipeline, select the Delaunay3D filter and add a Stream Tracer filter. You can find this under Filters \u2192 Common \u2192 Stream Tracer. Do not press Apply just yet.</p> <p>\u25b6 The Stream Tracer filter has quite a few parameters. Important ones are integration settings and the seed location for the particles to trace. Change them to, for example, the values below and click Apply:</p> <p></p> <p>This will give you a set of lines, each representing a trace of a particle as it follows the flow in the tornado. </p> <p>\u25b6 Note the small red 3D axis and large sphere at the bottom of the tornado: this is the seed center (12,12,0) and given radius around which the traced particles start. You could experiment with different locations of the seed point to see how this influences the streamlines.</p> <p>Accidental sphere changes / Resetting a filter to last executed values</p> <p>The sphere shown can actually be moved by dragging in the 3D view, which alters the corresponding values set for the Point Cloud Parameters. Sometimes, during 3D interaction with the view you might accidentally change a widget, such as the sphere here (there's more widgets, for example to set the location of a clip plane), which changes filter parameters. In case you did not want this to happen you can reset a filter back to the values it used since the last Apply by using the Reset button at the top of the Properties panel.</p> <p>In most cases a widget can be hidden using a filter option, which prevents further interaction with it. Hiding the sphere here can be done by disabling the Show Sphere option under Point Cloud Parameters.</p> <p>\u25b6 To make the streamlines more visually appealing, we add another filter on the output of the StreamTracer filter, namely a Tube filter (Filters \u2192 Alphabetical \u2192 Tube). In the Tube filter's Properties tab, set the radius of the tubes to 0.1 and click Apply. Notice how this changes the appearance of the streamlines.</p> <p>\u25b6 At this point, we'd like to add the original data domain as an outline. For this, enable visibility of the TableToPoints filter and switch its representation to Outline.</p> <p>\u25b6 You can experiment with different colorings of the tubes, based on e.g. velocity, angular velocity or rotation. Use the coloring controls in the filters settings under Coloring for this.</p>"},{"location":"exercise2/#glyphs","title":"Glyphs","text":"<p>Finally, we'll add a different representation instead of the streamlines, called glyphs. Glyphs are simple and (usually) small 3D objects, like arrows or spheres. These glyphs get placed at each point position in a dataset to show a particular value. The glyphs are then colored, scaled and/or oriented based on scalar or vector values at the location. We'll use arrow glyphs to show the flow velocity magnitude and direction in the tornado.</p> <p>\u25b6 Hide all filter output, except the TableToPoints filter (the domain), by clicking the relevant eye icons. </p> <p>\u25b6 Select the Calculator2 filter and add a Glyph filter (Filters \u2192 Common \u2192 Glyph). Set the Glyph Type to Arrow, set the Orientation Array to Velocity (i.e. our computed velocity vectors) and Scale Mode to No scale array. Click Apply. </p> <p></p> <p>Point versus cell input</p> <p>Note that there is no need to base the Glyph filter on the Delaunay 3D output, as the Glyph filter works on 3D points, as in the original data set. This is unlike the Streamtracer filter needing cells, which we added using the Delaunay 3D filter.</p> <p>ParaView does provide generic <code>Point Data to Cell Data</code> and <code>Cell Data to Point Data</code> filters, to convert between the two using interpolation.</p> <p>You should now see a large number of arrows nicely distributed over the tornado dataset, indicating the direction of wind flow. As we have set the Scale Mode to Off all arrows are the same size, obscuring the insides and giving less of a visual clue to wind speed.</p> <p>Let's try to improve the overall visualization, to make it easier to interpret.</p> <p>\u25b6 Set the Scale Array to Velocity and the Scale Factor to 0.2 and press Apply. </p> <p>\u25b6 Make sure the coloring is set to VelocityMag and verify that the size and colors of a glyph arrow corresponds to its velocity value.</p> <p>You might wonder about the the number of glyphs placed, compared to the 25,000 points in the dataset. There is currently quite a large number of glyphs, and perhaps still too many to be effective. This doesn't help in the overall visual interpretation of the data, but we do need to balance getting enough coverage of the full dataset.</p> <p>\u25b6 The filter settings under Masking provide different modes options for the number and distribution of the glyphs placed. For example, see what happens when using Glyph Mode <code>Every Nth point</code> when you show a glyph for every 10th or 11th point. Or use 500 glyphs uniformly distributed (and why that means you need choose these kinds of parameters with care).</p> <p>\u25b6 A useful variant is to apply glyphs to the output of the Stream Trace filter (by creating a second Glyph filter). This is possible because the generated streamlines are themselves polygonal data, where each streamline consists of a Poly-Line cell that uses a set of 3D points. As a Glyph filter uses point positions to place glyphs we can place them for each streamline. Experiment with this, using different types of glyphs, like Sphere and Arrow. Also try coloring by IntegrationTime to verify the direction in which the streamlines where \"grown\".</p>"},{"location":"exercise3/","title":"Exercise 3 - Time-dependent Visualization","text":""},{"location":"exercise3/#animating-a-contour-value","title":"Animating a contour value","text":"<p>In this exercise, we are going to look at time-dependent data. The data we use for this was produced by a computer simulation of coral growth. The file used contains a 3D regular grid, where each grid cell has a scalar value that defines the time step when that grid cell was filled with the growing coral in the simulation. We are going to use the animation tools in ParaView to link data to a time sequence.</p> <p>\u25b6 If you currently have any data loaded use the disconnect button/icon or File \u2192 Disconnect to clean your current workspace.</p> <p>\u25b6 Open the file containing the coral data set, named ALT_PRPB001A.vtk, located in the data directory and click Apply.</p> <p>\u25b6 Add a Contour filter to this dataset and hit Apply again. </p> <p>You should now see a contour surface of one single time point in the growth of the coral. However, what we want is to have the contour plot change dynamically over time, showing us the growth of the coral over time.</p> <p>\u25b6 To achieve this, we need to open the Time Manager, which can be enabled from the main menu with View \u2192 Time Manager. In the time manager toolbar, set Number of frames to 100. </p> <p>Time Manager in older paraview versions</p> <p>In ParaView versions before 5.12 the Time Manager was called the Animation View, also available under the View menu. It is very similar to the Time Manager in 5.12, but not completely the same. So you might have to puzzle a bit how to use it in those older versions.</p> <p>\u25b6 Link the contour iso-surface value to the time sequence by using the blue + button right of the Contour1 and Isosurfaces dropdown menus.</p> <p></p> <p>\u25b6 Verify that this adds Contour1 to the timeline, directly under Animations as a second \"strip\" over the full length of the time bar. The values at the far left and right edges of the strip are the isosurface values used at those time points (1280 and 37120, respectively). These values are based on the min/max values from the input dataset, which can you verify using the Information tab of ALT_PRPB001A.vtk.</p> <p></p> <p>\u25b6 With the animation set up you can now use the playback buttons, either in the Time Manager or in the main toolbar, to play through the growth of the simulation. The double arrow button controls if the playback loops back to the start. When the animation is paused you can change the current time by clicking or dragging in the Time Sources bar.</p> <p></p> <p>\u25b6 Notice how the contour value used changes during animation playback.</p>"},{"location":"exercise3/#camera-animation","title":"Camera animation","text":"<p>Since the coral is a three-dimensional structure, it is nice to look at all sides of the structure over time. To do this, we will add a camera path that orbits around the coral as it animates.</p> <p>\u25b6 The coral grows in the -Y direction, which you can verify by looking at the small 3D axes in the lower-left of the 3D view. This is unfortunate, as by default the data is shown upside-down. We will set up a good view using the toolbar buttons for viewing directly along one of the coordinate axes. Press the +Z button to look along the +Z direction, with +Y up, then press the +90 button twice to rotate the view to -Y up.</p> <p></p> <p>\u25b6 Verify that the animation playback now shows the coral nicely growing upwards, with the data ground plane horizontal. </p> <p>Next, we will add a circular camera motion, a so-called orbit.</p> <p>\u25b6 In the Time Manager in the Animations row select Camera from the left-most drop-down box, and Follow Path in the dropdown right to it. Then create a camera animation strip by pressing the blue + button. This will add a Camera - RenderView1 strip. If you play the animation you will see the scene rotates over time, however, the data is shown upside down...</p> <p>So the default camera orbit needs some tweaking, as it is set up based on the current view. For example, the camera might be a bit too close to the coral object and we want to move it back a bit. </p> <p>\u25b6 Bring up the animation parameters by double-clicking on the Camera strip in the Time Manager, this will show the Animation Keyframes dialog.</p> <p></p> <p>There are currently only two key frames defined, for time 0 and time 1. We will change the values for time 0 to tweak the camera animation.</p> <p>\u25b6 Select the row for time 0 in the dialog and click Create Orbit. This will show the Create Orbit dialog. The Center value is the point around which the camera is rotated, Normal is the vector used for the rotation and the Origin is the initial camera position. Set the Normal value to be -Y up (0, -1, 0), or else the might be some weird rotation during the orbit. Normally, you will need to experiment in your own scenes to figure out correct values, but use the ones given below and press Ok.</p> <p></p> <p>\u25b6 Press Ok in the Animation Keyframes dialog to apply the new values. Play back the animation again and observe a nice rotation of the data as the coral grows.</p> <p>\u25b6 You can try experimenting with some different orbit parameters, to get visually different animations.</p> <p>Saving the animation to a video</p> <p>Although not part of this exercise, it is really easy at this point to save the animated view to a movie file from ParaView. Use File \u2192 Save Animation for this. You can either save to a sequence of images, or directly to a video file, such as a .avi or .ogv. In this case it's best to set the Frame Rate value to something like 20 (leading to a 5 second animation, as we have 100 frames).</p>"},{"location":"miscellaneous/","title":"Miscellaneous","text":""},{"location":"miscellaneous/#official-resources","title":"Official resources","text":"<p>The official ParaView User Guide is quite good and contain lots of detailed information on data analysis and visualization with ParaView. The section in the User Guide on the ParaView data model can be especially relevant to understand how filters operate, and how to represent different types of datasets within this model. More information on the different file readers supported by ParaView can be found here.</p> <p>The ParaView Reference Manual has more information on UI components and Python scripting.</p> <p>There's also a whole set of official tutorials, from basic UI elements and tasks, to more advanced data analysis, animations, Python scripting and much more.</p> <p>For asking questions, as well as a knowledge-base to search, there is the ParaView Discourse forum. Many ParaView developers are active on the forum, helping users with questions or problems.</p>"},{"location":"miscellaneous/#interface-guide","title":"Interface guide","text":""},{"location":"miscellaneous/#common-view-types","title":"Common view types","text":"<p>The type of the main view area can be changed, or even split into multiple view areas of the same or different types. The buttons marked View area controls above allow these changes. When adding a view area, or changing the main area (delete it with the <code>x</code> button first), a menu is shown with the different types of views that can be created:</p> <p></p> <p>Below, we describe a few of these.</p> <ul> <li> <p>The Render View is the default and most used one. It provides an interactive 3D view where output of filters is shown. Per filter a visual representation can be chosen, as well as visibility per filter in the pipeline.</p> <p></p> </li> <li> <p>The SpreadSheet View shows underlying point and cell output data (and other types) of a single filter. This can be really useful to understand the underlying data values, to see if your import worked correctly, if a Python script generates correct values, etc.</p> <p>Note that the data shown depends on the filter chosen under <code>Showing</code> in the top-left, not the currently selected filter in the pipeline.</p> <p></p> </li> <li> <p>Some filters output statistics in a bar chart. For example, when you add a Histogram filter a Bar Chart View automatically gets added, splitting the existing view.</p> <p></p> </li> <li> <p>Something similar is done for filters producing line charts. For example, the Plot Over Line filter adds a Line Chart View.</p> <p></p> </li> </ul>"},{"location":"miscellaneous/#memory-inspector","title":"Memory inspector","text":"<p>As shown in the interface guide above, the memory indicator at the bottom of the window shows total system memory used. When enabling <code>View &gt; Memory inspector</code> a panel is added that also shows the current ParaView memory usage for the data loaded and filtering applied, as well as the overall system memory usage.</p> <p></p> <p>When using ParaView's client-server mode this will also list the memory usage each of the server processes.</p>"},{"location":"miscellaneous/#statistics-inspector","title":"Statistics inspector","text":"<p>Enabling <code>View &gt; Statistics inspector</code> will add a panel showing high-level information on all different filters in the pipeline. This includes geometry size for rendering, which is something not shown in the regular Information panel.</p> <p></p>"},{"location":"miscellaneous/#tips","title":"Tips","text":""},{"location":"miscellaneous/#quick-filter-search","title":"Quick filter search","text":"<p>With <code>Ctrl+Space</code> you can bring up the filter quick search menu:</p> <p></p> <p>By typing the first few letters of a filter name, \"cont\" in the example above, you can quickly find and add a filter as a child of the currently selected filter in the pipeline browser. The greyed out filter names indicate that those particular filters are not compatible with the currently selected filter's output data type.</p>"},{"location":"miscellaneous/#getting-filter-help","title":"Getting filter help","text":"<p>Pressing the <code>?</code> button at the top of the current filter's properties will show a help screen for that filter type. It will contain a description of what the filter does, its input(s) and output(s), its settings, and any other useful information.</p> <p></p>"},{"location":"miscellaneous/#searching-and-finding-hidden-properties","title":"Searching and finding (hidden) properties","text":"<p>Most filters have quite a lot of properties. Some of them are even hidden by default, and will only get listed by enabling the advanced property toggle . </p> <p></p> <p>Luckily, properties can be filtered on (partial) names using the <code>Search ...</code> entry, and the resulting list will show all matching properties, including hidden ones.</p>"},{"location":"miscellaneous/#rename-a-filter","title":"Rename a filter","text":"<p>You can rename a filter by selecting it and then double-clicking it in the pipeline browser (or right-click and then <code>Rename</code>).</p> <p></p>"},{"location":"miscellaneous/#changing-a-filters-input","title":"Changing a filter's input","text":"<p>You can change the input of a filter in the pipeline by right-clicking on it and picking <code>Change input</code>. This will bring up a dialog for selecting the new upstream filter. </p> <p>Below is an example of changing the CSV source of a pipeline to an updated file. This can be useful if during loading of a state file one or more input files can't be found (e.g. due to a changed path).</p> <p></p> <p>After the new input is set the pipeline will re-execute to make itself up-to-date.</p>"},{"location":"miscellaneous/#linking-cameras-between-views","title":"Linking cameras between views","text":"<p>If you have split the main 3D view into two (more more) views you can easily link the cameras between the views: right-click on a view, click <code>Link camera...</code> and then click on the view whose camera you want to link to. Now, the view cameras will stay the same when you interact with either view.</p>"},{"location":"miscellaneous/#docking-the-output-messages-window","title":"Docking the Output Messages window","text":"<p>Sometimes, a lot of warning or error messages will keep showing up in the Output Messages window. Even if you click away the window it will reappear on the next message. One way to slightly improve the situation is to dock the window in the main UI. For this, use the little windows icon:</p> <p></p>"},{"location":"preparation/","title":"Preparation","text":"<p>In this course we are going to use ParaView. ParaView is an open-source, multi-platform scientific visualization and data analysis application. It is available for Windows, macOS and Linux, see below.</p> <p>We will use a number of datasets provided for the course in the exercises.</p> <p>Both data files as well as ParaView binaries for Windows and Linux can be downloaded and installed using the instructions below. This course assumes that you bring your own PC/laptop, or if not, that you can work together with someone else that does have a PC/laptop.</p>"},{"location":"preparation/#course-materials-share","title":"Course materials share","text":"<p>All course data files and slides can be found at here. There we also provide ParaView binaries for Windows and Linux (see below).</p>"},{"location":"preparation/#install-paraview","title":"Install ParaView","text":"<p>We provide ParaView binaries for Windows and Linux for this course in the course materials share (see above). For macOS there are many variations available on https://www.paraview.org/download/ and you probably know best yourself which one you need.</p> <p>This guide has been written for ParaView version 5.13. Between ParaView versions small differences in GUI (and functionality) exist, but most of what is written in these notes should be easy to apply to other versions of ParaView.</p> <p>OpenGL compatibility</p> <p>An important difference between ParaView version 5.x and earlier versions is that new ParaView versions require a GPU that supports OpenGL 3.2 or higher. Older PCs/laptops and systems with Intel Integrated graphics may not support this. There are two options to work around this limitation:</p> <ol> <li> <p>Start ParaView (5.x) from the command line with the --mesa flag, e.g. <code>paraview --mesa</code>. This enables CPU-based software rendering, i.e, not using a GPU at all. This will be slower than rendering on the GPU, but should work on any system.</p> </li> <li> <p>Use an older version of ParaView, like 4.4 (available from http://www.paraview.org/download). In this case the GUI and menu options will be slightly different in naming, placement and look compared that what is described in these notes.</p> </li> </ol> <p>Wayland on Linux</p> <p>There are long-standing known issues with ParaView 5.12 (and probably 5.13 as well) on systems using Wayland instead of X.org. Unfortunately, there is no easy workaround, other than recompiling ParaView yourself as indicated here. There's a bug report to track any developments on the ParaView side, and here for the VTK side.</p>"},{"location":"preparation/#download-and-extract-data","title":"Download and extract data","text":"<p>The ZIP files with data used in this workshop can be found in the course materials share linked above. Some of the data sets are provided by the University of Amsterdam.</p> <p>\u25b6 Unzip the data file to a directory of your choice. </p> <p>This directory will now have 4 datasets:</p> <ul> <li>headsq_masked.vtk: This is a CT scan of a boy's head. The data consists of a regular grid of 255x255x93 voxels, containing scalar values for the density of the scanned tissue. (The original dataset consisted of 93 separate files containing binary data, unsigned shorts with connectivity bits. For this course, we pre-processed the file for easier use).</li> <li>wervel.csv: This is a Comma Separated Value file, containing a list of 3D points and their velocity vectors, representing a snapshot of a simulation of a tornado. </li> <li>ALT_PRPB001A.vtk: This file contains the results of a simulation of a coral growth. The scalars in this file represent the time of growth of a coral (this data is provided courtesy of Jaap Kaandorp , Section Computational Science, UvA).</li> <li>SMRX.vtk: This is a flow simulation of a viscous fluid through a stationary mixer (this data is provided courtesy of Drona Kandhai, Section Computational Science, UvA).</li> </ul> <p>Under the Exercises section there are four exercises based on these datasets.</p>"},{"location":"privacy/","title":"Privacy and cookies","text":""},{"location":"privacy/#privacy","title":"Privacy","text":"<p>No personal information is gathered by SURF of visitors to this course website.</p>"},{"location":"privacy/#cookies","title":"Cookies","text":"<p>No cookies are used for the content published by SURF on this website, nor is any personal information about visits tracked by SURF. </p> <p>The underlying MkDocs content generation system uses the browser's session storage for storing general site-map data (called <code>/blender-course/.__sitemap</code>), which is sometimes reported as a cookie.</p>"},{"location":"privacy/#third-party-cookies","title":"Third-party cookies","text":"<p>This website is hosted through GitHub Pages, which might set third-party cookies in which case explicit permission needs to be granted by the user. See here for the GitHub privacy policy.</p>"},{"location":"scripting/","title":"Scripting","text":"<p>ParaView comes with a Python 3.x layer that can be used for several different tasks:</p> <ul> <li>Automating visualization workflows</li> <li>Storing the ParaView state</li> <li>Creating custom filters</li> <li>Creating custom plots</li> </ul> <p>Obviously, Python scripting is an extensive and technically-oriented topic, and we won't go deep into the peculiarities of learning the Python API in ParaView. The best resources for that are the official tutorials, starting with the Python &amp; Batch: ParaView &amp; Python chapter. A dedicated set of ParaView Python documentation is available here.</p> <p>Below, we discuss the different possibilities listed above in more detail.</p>"},{"location":"scripting/#interaction-tracing","title":"Interaction tracing","text":"<p>The easiest way to get started with Python scripting from the ParaView GUI is to use the Trace feature:</p> <ul> <li><code>Tools &gt; Start Trace</code>, adjust options in the Trace Options dialog as needed (usually the defaults are fine), press OK.</li> <li>Perform actions within the GUI (for example, load a data file, add a Contour filter, press Apply)</li> <li><code>Tools &gt; Stop Trace</code></li> </ul> <p>The Python Script Editor window will pop up and will show the Python code representing the actions you performed. The script will usually contain quite a few more things than you did yourself in the GUI, as it records low-level implicit actions as well, for example related to rendering and updating the views. </p> <p>In principle, the generated script is self-contained and can be run later using the methods described below, for example from the command-line. Or you can use a Python trace as a basis for writing your own Python code for creating automated ParaView workflows.</p> <p>Note the commented part at the end of the trace, containing different statements, for example to save a screenshot or animation. Uncommenting some of those statements can be an easy way to get a script that does something useful, based on a trace of actions.</p>"},{"location":"scripting/#saving-state","title":"Saving state","text":"<p>A different way to get a Python representation from your current scene state is to use <code>File &gt; Save State</code>, pick <code>Python state file</code> as file type, and then save to a .py file. In most cases the defaults in the Python State Options window are good enough.</p> <p>The written script usually contains quite a bit of code, as it represent not just the scene in terms of filters, but also the UI state and things like animation state.</p> <p>A Python state file (and trace as well) can later be re-executed using <code>File &gt; Load State</code>, picking <code>Python state file</code> as file type. Note that in this case the current state is not cleared, so the script will add to the current state.</p> <p>Also note that no data used by the filter pipeline is stored in the state, only the pipeline setup (i.e. filter types, parameters and connections). So when re-executing the state the data files referenced by pipeline filters need to be available for reading.</p> <p>ParaView State Files (.psvm)</p> <p>Using Python is one method for saving ParaView state, but the default is to use an XML-based format, with extension .pvsm. This file can also be written using <code>File &gt; Save State</code>, file type <code>ParaView state file</code>.</p> <p>Saving (XML) state in a screenshot file</p> <p>An interesting feature that was added in ParaView 5.12 is to save the current state as XML metadata (so not as Python) in a screenshot PNG file. This only works for screenshots in PNG file format.</p> <p>Use <code>File &gt; Save Screenshot</code>, pick PNG as image type, enter the file name, click OK and then in the Save Screenshot Options dialog enable <code>Embed ParaView State</code>. The resulting PNG file can then later be loaded with <code>File &gt; Load State</code>.</p> <p>The reason for mentioning such non-Python state here is that you can save such screenshots from Python by passing <code>EmbedParaViewState=1</code> to the <code>SaveScreenshot()</code> function, which might be interesting for scripts doing batch-oriented rendering.</p>"},{"location":"scripting/#command-line-execution","title":"Command-line execution","text":"<p>To run Python scripts from the command-line ParaView provides two executables, <code>pvpython</code> and <code>pvbatch</code>:</p> <ul> <li><code>pvpython</code> allows interactive use, as well as batch execution of Python scripts.</li> <li><code>pvbatch</code> is very similar to <code>pvpython</code>, but it can only run scripts, there's no interactive prompt. However, it can be run under MPI to allow parallel processing of larger datasets, something that isn't possible with <code>pvpython</code>.</li> </ul> <p>Remote and parallel usage</p> <p>There's another difference between <code>pvpython</code> and <code>pvbatch</code> in that <code>pvpython</code> can be used to connect to a running ParaView server, thereby allowing remote (client-server) usage. In contrast, <code>pvbatch</code> always runs locally on the system and cannot connect as a client to a ParaView server.</p> <p><code>pvpython</code> contains GUI-related components, so it might show windows when executing certain commands (such as making a screenshot).</p> <pre><code>$ pvpython\nPython 3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from paraview.simple import *\n&gt;&gt;&gt; wavelet = Wavelet()\n&gt;&gt;&gt; Show(wavelet)\n[openvkl] application requested ISPC device width 8via device name cpu_8\n[openvkl] CPU device instantiated with width: 8, ISA: AVX2\n&lt;paraview.servermanager.UniformGridRepresentation object at 0x71244c38f8d0&gt;\n&gt;&gt;&gt; contour = Contour(Input=wavelet)\n&gt;&gt;&gt; contour.Isosurfaces = [200.0]\n&gt;&gt;&gt; Show(contour)\n&lt;paraview.servermanager.GeometryRepresentation object at 0x71244c3f8550&gt;\n&gt;&gt;&gt; SaveScreenshot(\"iso.png\")\n# Shows window, which stays open\nTrue\n&gt;&gt;&gt; \n</code></pre> <p>The above shows interactive usage, while you can also run a script from a file, just like regular <code>python</code>:</p> <pre><code>$ cat wavelet-contour.py \nfrom paraview.simple import *\n\nwavelet = Wavelet()\nShow(wavelet)\n\ncontour = Contour(Input=wavelet)\ncontour.Isosurfaces = [200.0]\nShow(contour)\n\nSaveScreenshot(\"iso.png\")\n\n$ pvpython wavelet-contour.py \n[openvkl] application requested ISPC device width 8via device name cpu_8\n[openvkl] CPU device instantiated with width: 8, ISA: AVX2\n# Briefly shows windows, which closes when the script is done\n</code></pre> <p>Offscreen rendering</p> <p>Both <code>pvpython</code> and <code>pvbatch</code> support a <code>--force-offscreen-rendering</code> option, which avoids creating windows when rendering images. Depending on the system configuration where you're running ParaView this might or might not work. </p>"},{"location":"scripting/#gui-support","title":"GUI support","text":"<p>Output messages area</p> <p>Enable <code>View &gt; Output Messages</code> and dock it somewhere in the GUI so you can see any output or errors when executing Python scripts.</p>"},{"location":"scripting/#the-python-shell","title":"The Python Shell","text":"<p>To work more interactively with scripting you can enable <code>View &gt; Python Shell</code>. This adds an interactive Python prompt to the UI. There, you can enter Python commands and see the results right away.</p> <p></p> <p>The Python Shell also allows you to easily run Python script files using the <code>Run Script</code> button. Note that these get executed within the current scene state, the scene is not cleared first.</p>"},{"location":"scripting/#the-python-script-editor-window","title":"The Python Script Editor window","text":"<p>ParaView contains a dedicated Python editor window, which can be shown using <code>Tools &gt; Python Script Editor</code>. It also gets shown automatically to hold any traces made, as described earlier.</p> <p>The editor window provides a syntax-highlighted area for Python code, and has limited editing facilities. It also appears to be a bit buggy in certain uses, so using an external editor might be preferable.</p>"},{"location":"scripting/#programmable-filter","title":"Programmable Filter","text":"<p>The Programmable Filter (and similar Programmable Source) allows you to create a custom filter using Python. A Programmable Filter is a pipeline filter in all respects. It takes an input dataset and produces an output dataset of a chosen type. It can compute any function that you can write in Python (including use of packages). There's special support for making NumPy usage easier with Paraview (and VTK) data types.</p> <p>Let's say we wanted to create a filter that normalizes a dataset using the minimum and maximum value ranges. Below is Python code for such a programmable filter, applied to a dataset having point data array \"RTData\":</p> <pre><code>data_in = inputs[0].PointData[\"RTData\"]\nminval = numpy.min(data_in)\nmaxval = numpy.max(data_in)\n\ndata_out = (data_in - minval) / (maxval - minval)\noutput.PointData.append(data_out, \"Normalized\")\n</code></pre> <p>In this case, the upstream filter whose data we are processing is a Wavelet source. The <code>inputs</code> array contains all input ports of the programmable filter, of which there is just one here, which is receiving data from the upstream wavelet source. On that input port we retrieve the point data array named \"RTData\" (which a wavelet value for each 3D point). We compute min and max values and normalize the input array in one go, thanks to NumPy, and then set that as the point data output array called \"Normalized\".</p> <p>Here's the GUI showing the pipeline, the code set on the programmable filter, as well as isosurfaces halfway in the data range, for the original wavelet dataset (left), and the normalized one (right). Note the different color map ranges.</p> <p></p> <p>As you can see, it is quite easy to write a custom filter. As long as you can express whatever operation you need in Python. And for large datasets the operation needs to be efficient to execute, unless you don't mind waiting a bit.</p> <p>However, in this example the output data was a simple 3D array, containing per-point values, so that made the operation easy to express. But when you want to produce more exotic data types, such as 3D geometry, you will need to create data that adheres to the ParaView/VTK data model.</p> <p>Editing the filter code can also be a bit cumbersome. The code area does not show a lot of lines, so you might need to scroll for longer scripts. With the  button you can open the Script Editor window in linked mode: any changes you make in the editor window will be applied to the filter code, and vice versa. You can even save the code to a script file from the Editor Window. However, it forces you to use the separate Editor Window, which will overlap the main GUI.</p>"},{"location":"scripting/#python-view","title":"Python View","text":"<p>Finally, there is the Python View: a view area that shows an image that you create using a Python script. It can be used for custom plots, or any other type of image display. It is not the most efficient to use, unfortunately:</p> <ul> <li>It does not operate as a filter receiving upstream data, so the script needs to be hardcode explicitly which data it needs. This introduces a dependency between scene state and view code.</li> <li>Updating the view (i.e. when you change the script code) can't be forced with a button, but is handled implicitly when the Python View is redrawn.</li> <li>Error handling seems to be a bit buggy, with Python syntax errors causing ParaView to crash sometimes.</li> </ul> <p>For these reasons, we don't recommend using the Python View, unless you really need custom plot types within ParaView.</p>"}]}